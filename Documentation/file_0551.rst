What are toolchains?
--------------------

A toolchain is a collection of tools to build (HPC) software
consistently. It consists of

-  compilers for C/C++ and Fortran,
-  a communications library (MPI), and
-  mathematical libraries (linear algebra, FFT).

Toolchains are versioned, and refreshed twice a year. All software
available on the cluster is rebuild when a new version of a toolchain is
defined to ensure consistency. Version numbers consist of the year of
their definition, followed by either ``a`` or ``b``, e.g., ``2014a``.
Note that the software components are not necessarily the most recent
releases, rather they are selected for stability and reliability.

Two toolchain flavors are standard across the VSC on all machines that
can support them: ``intel`` (based on Intel software components) and
``foss`` (based on free and open source software).

It may be of interest to note that the Intel C/C++ compilers are more
strict with respect to the standards than the GCC C/C++ compilers, while
for Fortran, the GCC Fortran compiler tracks the standard more closely,
while Intel's Fortran allows for many extensions added during Fortran's
long history. When developing code, one should always build with both
compiler suites, and eliminate all warnings.

On average, the Intel compiler suite produces executables that are 5 to
10 % faster than those generated using the GCC compiler suite. However,
for individual applications the differences may be more significant with
sometimes significantly faster code produced by the Intel compilers
while on other applications the GNU compiler may produce much faster
code.

Additional toolchains may be defined on specialised hardware to extract
the maximum performance from that hardware.

-  On Cerebro, the SGI UV shared memory system at the KU Leuven, you
   need to use the SGI MPI-library (called MPT for Message Passing
   Toolkit) to get the maximum performance from the interconnect (which
   offers hardware acceleration for some MPI functions). On that
   machine, two additional toolchains are defined, ``intel-mpt`` and
   ``foss-mpt``, equivalent to the standard ``intel`` and ``foss``
   toolchains respectively but with the MPI library replaced with MPT.

Intel toolchain
---------------

The ``intel`` toolchain consists almost entirely of software components
developed by Intel. When building third-party software, or developing
your own, load the module for the toolchain:

::

   $ module load intel/<version>

where ``<version>`` should be replaced by the one to be used, e.g., .
See the documentation on the software module system for more details.

Starting with the ``2014b`` toolchain, the GNU compilers are also
included in this toolchain as the Intel compilers use some of the
libraries and as it is possible (though some care is needed) to link
code generated with the Intel compilers with code compiled with the GNU
compilers.

Compilers: Intel and Gnu
~~~~~~~~~~~~~~~~~~~~~~~~

Three compilers are available:

-  C: ``icc``
-  C++: ``icpc``
-  Fortran: ``ifort``

Recent versions of

For example, to compile/link a Fortran program ``fluid.f90`` to an
executable ``fluid`` with architecture specific optimization, use:

::

   $ ifort  -O2  -xhost  -o fluid  fluid.f90

Documentation on Intel compiler flags and options is `provided by
Intel <\%22https://software.intel.com/sites/default/files/Compiler_QRG_2013.pdf\%22>`__.
Do not forget to *load the toolchain module* first!

Intel OpenMP
^^^^^^^^^^^^

The compiler switch to use to compile/link OpenMP C/C++ or Fortran code
is ``-openmp``. For example, to compile/link a OpenMP C program
``scatter.c`` to an executable ``scatter`` with architecture specific
optimization, use:

::

   $ icc  -openmp  -O2  -xhost  -o scatter  scatter.c

Remember to specify as many processes per node as the number of threads
the executable is supposed to run. This can be done using the ``ppn``
resource, e.g., ``-l nodes=1:ppn=10`` for an executable that should be
run with 10 OpenMP threads. The number of threads should not exceed the
number of cores on a compute node.

Communication library: Intel MPI
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For the intel toolchain, ``impi``, i.e., Intel MPI is used as the
communications library. To compile/link MPI programs, wrappers are
supplied, so that the correct headers and libraries are used
automatically. These wrappers are:

-  C: ``mpiicc``
-  C++: ``mpiicpc``
-  Fortran: ``mpiifort``

Note that the *names differ* from those of other MPI implementations.
The compiler wrappers take the same options as the corresponding
compilers.

Using the Intel MPI compilers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For example, to compile/link a C program ``thermo.c`` to an executable
``thermodynamics`` with architecture specific optimization, use:

::

   $ mpiicc -O2  -xhost  -o thermodynamics  thermo.c

Extensive documentation is `provided by
Intel <\%22https://software.intel.com/en-us/articles/intel-mpi-library-documentation\%22>`__.
Do not forget to *load the toolchain module* first.

Running an MPI program with Intel MPI
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Note that an MPI program must be run with the exact same version of the
toolchain as it was originally build with. The listing below shows a PBS
job script ``thermodynamics.pbs`` that runs the ``thermodynamics``
executable.

::

   #!/bin/bash -l
   module load intel/<version>
   cd $PBS_O_WORKDIR n_proc=$( cat $PBS_NODEFILE  |  wc  -l )
   mpirun  -np $n_proc  ./thermodynamics

The number of processes is computed from the length of the node list in
the ``$PBS_NODEFILE`` file, which in turn is specified as a resource
specification when submitting the job to the queue system.

Intel mathematical libraries
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The Intel Math Kernel Library (MKL) is a comprehensive collection of
highly optimized libraries that form the core of many scientific HPC
codes. Among other functionality, it offers:

-  BLAS (Basic Linear Algebra Subsystem), and extensions to sparse
   matrices
-  Lapack (Linear algebra package) and ScaLAPACK (the distributed memory
   version)
-  FFT-routines including routines compatible with the FFTW2 and FFTW3
   libraries (Fastest Fourier Transform in the West)
-  Various vector functions and statistical functions that are optimised
   for the vector instruction sets of all recent Intel processor
   families

Intel offers `extensive
documentation <\%22https://software.intel.com/en-us/articles/intel-math-kernel-library-documentation\%22>`__
on this library and how to use it.

There are two ways to link the MKL library:

-  If you use icc, icpc or ifort to link your code, you can use the -mkl
   compiler option:

   -  -mkl=parallel or -mkl: Link the multi-threaded version of the
      library.
   -  -mkl=sequential: Link the single-threaded version of the library
   -  -mkl=cluster: Link the cluster-specific and sequential library,
      i.e., ScaLAPACK will be included, but assumes one process per core
      (so no hybrid MPI/multi-threaded approach)

   The Fortran95 interface library for lapack is not automatically
   included though. You'll have to specify that library seperately. You
   can get the value from the `MKL Link Line
   Advisor <\%22https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor\%22>`__,
   see also the next item.
-  Or you can specify all libraries explictly. To do this, it is
   strongly recommended to use Intel's `MKL Link Line
   Advisor <\%22https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor/\%22>`__,
   and will also tell you how to link the MKL library with code
   generated with the GNU and PGI compilers.
   **Note:** On most VSC systems, the variable MKLROOT has a different
   value from the one assumed in the Intel documentation. Wherever you
   see ``$(MKLROOT)`` you may have to replace it with
   ``$(MKLROOT)/mkl``.

MKL also offers a very fast streaming pseudorandom number generator, see
the documentation for details.

Intel toolchain version numbers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

+-----------+-----------------+-----------------+-----------------+
|           | 2014a           | 2014b           | 2015a           |
+===========+=================+=================+=================+
| icc       | 13.1.3 20130607 | 13.1.3 20130607 | 15.0.1 20141023 |
+-----------+-----------------+-----------------+-----------------+
| icpc      | 13.1.3 20130607 | 13.1.3 20130607 | 15.0.1 20141023 |
+-----------+-----------------+-----------------+-----------------+
| ifort     | 13.1.3 20130607 | 13.1.3 20130607 | 15.0.1 20141023 |
+-----------+-----------------+-----------------+-----------------+
| Intel MPI | 4.1.3.045       | 4.1.3.049       | 5.0.2.044       |
+-----------+-----------------+-----------------+-----------------+
| Intel MKL | 11.1.1.106      | 11.1.2.144      | 11.2.1.133      |
+-----------+-----------------+-----------------+-----------------+
| GCC       | /               | 4.8.3           | 4.9.2           |
+-----------+-----------------+-----------------+-----------------+

Further information on Intel tools
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

-  All Intel documentation of recent software versions is available in
   the `Intel Software Documentation
   Library <\%22https://software.intel.com/en-us/intel-software-technical-documentation\%22>`__.
   The documentation is typically available for the most recent version
   and sometimes one older version of te compiler and libraries.
-  MKL

   -  `Link page to the documentation of the most recent version on the
      Intel web
      site <\%22https://software.intel.com/en-us/articles/intel-math-kernel-library-documentation/\%22>`__
   -  `MKL Link Line
      Advisor <\%22https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor\%22>`__
   -  Older versions:

      -  intel/2014a and intel/2014b toolchain, MKL 11.1: `User's
         Guide <\%22https://software.intel.com/en-us/mkl_11.1_ug_lin\%22>`__
         and `Reference
         Guide <\%22https://software.intel.com/en-us/mkl_11.1_ref\%22>`__

-  `Generic BLAS/LAPACK/ScaLAPACK
   documentation <\%22/cluster-doc/development/blas-lapack#Links\%22>`__

FOSS toolchain
--------------

The ``foss`` toolchain consists entirely of free and open source
software components. When building third-party software, or developing
your own, load the module for the toolchain:

::

   $ module load foss/<version>

where ``<version>`` should be replaced by the one to be used, e.g.,
``2014a``. See the documentation on the software module system for more
details.

Compilers: GNU
~~~~~~~~~~~~~~

Three GCC compilers are available:

-  C: ``gcc``
-  C++: ``g++``
-  Fortran: ``gfortran``

For example, to compile/link a Fortran program ``fluid.f90`` to an
executable ``fluid`` with architecture specific optimization for
processors that support AVX instructions, use:

::

   $ gfortran -O2 -march=corei7-avx -o fluid fluid.f90

Documentation on GCC compiler flags and options is available on the
`project's website <\%22http://gcc.gnu.org/onlinedocs/\%22>`__. Do not
forget to load the toolchain module first!

GCC OpenMP
^^^^^^^^^^

The compiler switch to use to compile/link OpenMP C/C++ or Fortran code
is ``-fopenmp``. For example, to compile/link a OpenMP C program
``scattter.c`` to an executable ``scatter`` with optimization for
processors that support the AVX instruction set, use:

::

   $ gcc -fopenmp -O2 -march=corei7-avx -o scatter scatter.c

Remember to specify as many processes per node as the number of threads
the executable is supposed to run. This can be done using the ``ppn``
resource, e.g., ``-l nodes=1:ppn=10`` for an executable that should be
run with 10 OpenMP threads. The number of threads should not exceed the
number of cores on a compute node.

Note that the OpenMP runtime library used by GCC is of inferior quality
when compared to Intel's, so developers are strongly encouraged to use
the ```intel`` toolchain <\%22#intel-toolchain\%22>`__ when
developing/building OpenMP software.

Communication library: OpenMPI
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For the ``foss`` toolchain, OpenMPI is used as the communications
library. To compile/link MPI programs, wrappers are supplied, so that
the correct headers and libraries are used automatically. These wrappers
are:

-  C: ``mpicc``
-  C++: ``mpic++``
-  Fortran: ``mpif77``, ``mpif90``

The compiler wrappers take the same options as the corresponding
compilers.

Using the MPI compilers from OpenMPI
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For example, to compile/link a C program ``thermo.c`` to an executable
``thermodynamics`` with architecture specific optimization for the AVX
instruction set, use:

::

   $ mpicc -O2 -march=corei7-avx -o thermodynamics thermo.c

Extensive documentation is `provided on the project's
website <\%22http://www.open-mpi.org/doc/\%22>`__. Do not forget to load
the toolchain module first.

Running an OpenMPI program
^^^^^^^^^^^^^^^^^^^^^^^^^^

Note that an MPI program must be run with the exact same version of the
toolchain as it was originally build with. The listing below shows a PBS
job script ``thermodynamics.pbs`` that runs the ``thermodynamics``
executable.

::

   #!/bin/bash -l 
   module load intel/<version> 
   cd $PBS_O_WORKDIR 
   mpirun ./thermodynamics

The hosts and number of processes is retrieved from the queue system,
that gets this information from the resource specification for that job.

FOSS mathematical libraries
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The foss toolchain contains the basic HPC mathematical libraries, it
offers:

-  `OpenBLAS <\%22http://www.openblas.net/\%22>`__ (Basic Linear Algebra
   Subsystem)
-  `Lapack <\%22http://www.netlib.org/lapack/\%22>`__\ (Linear Algebra
   PACKage)
-  ScaLAPACK (Scalable Linear Algebra PACKage)
-  `FFTW <\%22http://www.fftw.org/\%22>`__ (Fastest Fourier Transform in
   the West)

Version numbers FOSS toolchain
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

+-----------+-------+-------+--------+
|           | 2014a | 2014b | 2015a  |
+===========+=======+=======+========+
| GCC       | 4.8.2 | 4.8.3 | 4.9.2  |
+-----------+-------+-------+--------+
| OpenMPI   | 1.6.5 | 1.8.1 | 1.8.3  |
+-----------+-------+-------+--------+
| OpenBLAS  | 0.2.8 | 0.2.9 | 0.2.13 |
+-----------+-------+-------+--------+
| LAPACK    | 3.5.0 | 3.5.0 | 3.5.0  |
+-----------+-------+-------+--------+
| ScaLAPACK | 2.0.2 | 2.0.2 | 2.0.2  |
+-----------+-------+-------+--------+
| FFTW      | 3.3.3 | 3.3.4 | 3.3.4  |
+-----------+-------+-------+--------+

Further information on FOSS components
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

-  `Overview of GCC manuals (all
   versions) <\%22https://gcc.gnu.org/onlinedocs/\%22>`__
-  OpenMPI documentation

   -  `1.8.x (foss/2014b and
      foss/2015a) <\%22http://www.open-mpi.org/doc/v1.8/\%22>`__
   -  `1.6.x (foss/2014a) <\%22http://www.open-mpi.org/doc/v1.6/\%22>`__

-  The `OpenBLAS project page <\%22http://www.openblas.net/\%22>`__ and
   `documentation
   Wiki <\%22https://github.com/xianyi/OpenBLAS/wiki\%22>`__
-  `Generic BLAS/LAPACK/ScaLAPACK
   documentation <\%22/cluster-doc/development/blas-lapack#Links\%22>`__

"
