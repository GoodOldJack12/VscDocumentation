UAntwerpen has two clusters. `leibniz <\%22#leibniz\%22>`__ and
`hopper <\%22#hopper\%22>`__, `Turing <\%22#turing\%22>`__, an older
cluster, has been retired in the early 2017.

Local documentation
-------------------

-  `Slides of the information sessions on \\"Transitioning to Leibniz
   and CentOS 7\" (PDF) <\%22/assets/1323\%22>`__
-  `The 2017a toolchain at
   UAntwerp <\%22/infrastructure/hardware/hardware-ua/toolchain-2017a\%22>`__:
   In preparation of the integration of Leibniz in the UAntwerp
   infrastructure, the software stack has been rebuild in the 2017a
   toolchain. Several changes have been made to the naming and the
   organization of the toolchains. The toolchain is now loaded by
   default on Hopper, and is the main toolchain on Leibniz and later
   also on Hopper after an OS upgrade.
-  `The Intel compiler
   toolchains <\%22/infrastructure/hardware/hardware-ua/intel\%22>`__:
   From the 2017a toolchain on, the setup of the toolchains differs on
   the UAntwerp clusters differs from most other VSC systems. We have
   set up the Intel compilers, including all libraries, in a single
   directory structure as intended by Intel. Some scripts, including
   compiler configuration scripts, expect this setup to work properly.
-  `Licensed software at
   UAntwerp <\%22https://www.vscentrum.be/infrastructure/hardware/hardware-ua/licensed-software\%22>`__:
   Some software has a restricted license and is not available to all
   users. This page lists some of those packages and explains for some
   how you can get access to the package.
-  Special nodes:

   -  `GUI programs and remote visualization
      node <\%22/infrastructure/hardware/hardware-ua/visualization\%22>`__:
      Leibniz offers a remote visualization node with software stack
      based on TurboVNC and OpenGL. All other login nodes offer the same
      features minus the OpenGL support (so applications have to link to
      a OpenGL software emulation library).
   -  `GPU computing
      nodes <\%22/infrastructure/hardware/hardware-ua/gpu-computing\%22>`__
   -  `Xeon Phi
      testbed <\%22/infrastructure/hardware/hardware-ua/xeonphi\%22>`__

-  `Information for Leibniz test
   users <\%22/infrastructure/hardware/hardware-ua/leibniz-instructions\%22>`__

Leibniz
-------

Leibniz was installed in the spring of 2017. It is a NEC system
consisting of 152 nodes with 2 14-core intel
`E5-2680v4 <\%22https://ark.intel.com/nl/products/91754/Intel-Xeon-Processor-E5-2680-v4-35M-Cache-2_40-GHz\%22>`__
Broadwell generation CPUs connected through a EDR InfiniBand network.
144 of these nodes have 128 GB RAM, the other 8 have 256 GB RAM. The
nodes do not have a sizeable local disk. The cluster also contains a
node for visualisation, 2 nodes for GPU computing (NVIDIA Psscal
generation) and one node with an Intel Xeon Phi expansion board.

Access restrictions
~~~~~~~~~~~~~~~~~~~

Access ia available for faculty, students (master's projects under
faculty supervision), and researchers of the AUHA. The cluster is
integrated in the VSC network and runs the standard VSC software setup.
It is also available to all VSC-users, though we appreciate that you
contact the UAntwerpen support team so that we know why you want to use
the cluster.

Jobs can have a maximal execution wall time of 3 days (72 hours).

Hardware details
~~~~~~~~~~~~~~~~

-  Interactive work:

   -  2 login nodes. These nodes have a very similar architecture to the
      compute nodes.

   -  1 visualisation node with a NVIDIA P5000 GPU. This node is meant
      to be used for interactive visualizations (`specific
      instructions <\%22/infrastructure/hardware/hardware-ua/visualization\%22>`__).

-  Compute nodes:

   -  144 nodes with 2 14-core Intel
      `E5-2680v4 <\%22https://ark.intel.com/nl/products/91754/Intel-Xeon-Processor-E5-2680-v4-35M-Cache-2_40-GHz\%22>`__
      CPUs (Broadwell generation, 2.4 GHz) and 128 GB RAM.
   -  8 nodes with 2 14-core Intel
      `E5-2680v4 <\%22https://ark.intel.com/nl/products/91754/Intel-Xeon-Processor-E5-2680-v4-35M-Cache-2_40-GHz\%22>`__
      CPUs (Broadwell generation, 2.4 GHz) and 256 GB RAM.
   -  2 nodes with 2 14-core Intel
      `E5-2680v4 <\%22https://ark.intel.com/nl/products/91754/Intel-Xeon-Processor-E5-2680-v4-35M-Cache-2_40-GHz\%22>`__
      CPUs (Broadwell generation, 2.4 GHz), 128 GB RAM and two NVIDIA
      Tesla P100 GPUs with 16 GB HBM2 memory per GPU (delivering a peak
      performance of 4.7 TFlops in double precision per GPU) (`specific
      instructions <\%22/infrastructure/hardware/hardware-ua/gpu-computing\%22>`__).
   -  1 node with 2 14-core Intel
      `E5-2680v4 <\%22https://ark.intel.com/nl/products/91754/Intel-Xeon-Processor-E5-2680-v4-35M-Cache-2_40-GHz\%22>`__
      CPUs (Broadwell generation, 2.4 GHz), 128 GB RAM and Intel Xeon
      Phi 7220P PCIe card with 16 GB of RAM (`specific
      instructions <\%22/infrastructure/hardware/hardware-ua/xeonphi\%22>`__).
   -  All nodes are connected through a EDR InfiniBand network
   -  All compute nodes contain only a small SSD drive. This implies
      that swapping is not possible and that users should preferably use
      the main storage for all temporary files also.

-  Storage: The cluster relies on the storage provided by Hopper (a 100
   TB DDN SFA7700 system with 4 storage servers).

Login infrastructure
~~~~~~~~~~~~~~~~~~~~

Direct login is possible to both login nodes and to the visualization
node.

-  From outside the VSC network: Use the external interface names.
   Currently, one needs to be on the network of UAntwerp or some
   associated institutions to be able to access the external interfaces.
   Otherwise a VPN connection is needed to the UAntwerp network.
-  From inside the VSC network (e.g., another VSC cluster): Use the
   internal interface names.

+--------------------+------------------------------+----------------------------+
|                    | External interface           | Internal interface         |
+--------------------+------------------------------+----------------------------+
| Login generic      | login-leibniz.uantwerpen.be  |                            |
+--------------------+------------------------------+----------------------------+
| Login              | login1-leibniz.uantwerpen.be | ln1.leibniz.antwerpen.vsc  |
|                    | login2-leibniz.uantwerpen.be | ln2.leibniz.antwerpen.vsc  |
+--------------------+------------------------------+----------------------------+
| Visualisation node | viz1-leibniz.uantwerpen.be   | viz1.leibniz.antwerpen.vsc |
+--------------------+------------------------------+----------------------------+

Storage organization
~~~~~~~~~~~~~~~~~~~~

See `the section on the storage organization of
hopper <\%22#hopper-storage\%22>`__.

Characteristics of the compute nodes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

| Since leibniz is currently a homogenous system with respect to CPU
  type and interconnect, it is not needed to specify the corresponding
  properties (see also the page on `specifying resources, output files
  and
  notifications <\%22https://www.vscentrum.be/cluster-doc/running-jobs/specifying-requirements\%22>`__).

However, to make it possible to write job scripts that can be used on
both hopper and leibniz (or other VSC clusters) and to prepare for
future extensions of the cluster, the following features are defined:

+-----------------------------------+-----------------------------------+
| property                          | explanation                       |
+===================================+===================================+
| broadwell                         | only use Intel processors from    |
|                                   | the Broadwell family (E5-XXXv4)   |
|                                   | (Not needed at the moment as this |
|                                   | is the only CPU type)             |
+-----------------------------------+-----------------------------------+
| ib                                | use InfiniBand interconnect (not  |
|                                   | needed at the moment as all nodes |
|                                   | are connected to the InfiniBand   |
|                                   | interconnect)                     |
+-----------------------------------+-----------------------------------+
| mem128                            | use nodes with 128 GB RAM         |
|                                   | (roughly 112 GB available). This  |
|                                   | is the majority of the nodes on   |
|                                   | leibniz.                          |
+-----------------------------------+-----------------------------------+
| mem256                            | use nodes with 256 GB RAM         |
|                                   | (roughly 240 GB available). This  |
|                                   | property is useful if you submit  |
|                                   | a batch of jobs that require more |
|                                   | than 4 GB of RAM per processor    |
|                                   | but do not use all cores and you  |
|                                   | do not want to use a tool to      |
|                                   | bundle jobs yourself such as      |
|                                   | Worker, as it helps the scheduler |
|                                   | to put those jobs on nodes that   |
|                                   | can be further filled with your   |
|                                   | jobs.                             |
+-----------------------------------+-----------------------------------+

These characteristics map to the following nodes on Hopper:

+-------+-------+-------+-------+-------+-------+-------+-------+-------+
| Type  | CPU   | Inter | #     | #     | #     | insta | avail | local |
| of    | type  | conne | nodes | physi | logic | lled  | mem   | disc  |
| node  |       | ct    |       | cal   | al    | mem   | (per  |       |
|       |       |       |       | cores | cores | (per  | node) |       |
|       |       |       |       | (per  | (per  | node) |       |       |
|       |       |       |       | node) | node) |       |       |       |
+=======+=======+=======+=======+=======+=======+=======+=======+=======+
| broad | `Xeon | IB-ED | 144   | 28    | 28    | 128   | 112   | ~25   |
| well: | E5-26 | R     |       |       |       | GB    | GB    | GB    |
| ib:me | 80v4  |       |       |       |       |       |       |       |
| m128  | <\%22 |       |       |       |       |       |       |       |
|       | https |       |       |       |       |       |       |       |
|       | ://ar |       |       |       |       |       |       |       |
|       | k.int |       |       |       |       |       |       |       |
|       | el.co |       |       |       |       |       |       |       |
|       | m/pro |       |       |       |       |       |       |       |
|       | ducts |       |       |       |       |       |       |       |
|       | /7527 |       |       |       |       |       |       |       |
|       | 7\%22 |       |       |       |       |       |       |       |
|       | >`__  |       |       |       |       |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+-------+
| broad | `Xeon | IB-ED | 8     | 28    | 28    | 256   | 240   | ~25   |
| well: | E5-26 | R     |       |       |       | GB    | GB    | GB    |
| ib:me | 80v4  |       |       |       |       |       |       |       |
| m256  | <\%22 |       |       |       |       |       |       |       |
|       | https |       |       |       |       |       |       |       |
|       | ://ar |       |       |       |       |       |       |       |
|       | k.int |       |       |       |       |       |       |       |
|       | el.co |       |       |       |       |       |       |       |
|       | m/pro |       |       |       |       |       |       |       |
|       | ducts |       |       |       |       |       |       |       |
|       | /7527 |       |       |       |       |       |       |       |
|       | 7\%22 |       |       |       |       |       |       |       |
|       | >`__  |       |       |       |       |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+-------+

Hopper
------

Hopper is the current UAntwerpen cluster. It is a HP system consisting
of 168 nodes with 2 10-core Intel E5-2680v2 Ivy Bridge generation CPUs
connected through a FDR10 InfiniBand network. 144 nodes have a memory
capacity of 64 GB while 24 nodes have 256 GB of RAM memory. The system
has been reconfigured to have a software setup that is essentially the
same as on Leibniz.

.. _access-restrictions-1:

Access restrictions
~~~~~~~~~~~~~~~~~~~

Access ia available for faculty, students (master's projects under
faculty supervision), and researchers of the AUHA. The cluster is
integrated in the VSC network and runs the standard VSC software setup.
It is also available to all VSC-users, though we appreciate that you
contact the UAntwerpen support team so that we know why you want to use
the cluster.

Jobs can have a maximal execution wall time of 3 days (72 hours).

.. _hardware-details-1:

Hardware details
~~~~~~~~~~~~~~~~

-  4 login nodes accessible through the generic name
   ``login.hpc.uantwerpen.be``.

   -  Use this hostname if you read *vsc.login.node* in the
      documentation and want to connect to this login node

-  Compute nodes

   -  144 (96 installed in the first round, 48 in the first expansion)
      nodes with 2 10-core Intel
      `E5-2680v2 <\%22https://ark.intel.com/products/75277\%22>`__ CPUs
      (Ivy Bridge generation) with 64 GB of RAM.
   -  24 nodes with 2 10-core Intel
      `E5-2680v2 <\%22https://ark.intel.com/products/75277\%22>`__ CPUs
      (Ivy Bridge generation) with 256 GB of RAM.
   -  All nodes are connected through an InfiniBand FDR10 interconnect.

-  Storage

   -  Storage is provided through a 100 TB DDN SFA7700 disk array with 4
      storage servers.

.. _login-infrastructure-1:

Login infrastructure
~~~~~~~~~~~~~~~~~~~~

Direct login is possible to both login nodes and to the visualization
node.

-  From outside the VSC network: Use the external interface names.
   Currently, one needs to be on the network of UAntwerp or some
   associated institutions to be able to access the external interfaces.
   Otherwise a VPN connection is needed to the UAntwerp network.
-  From inside the VSC network (e.g., another VSC cluster): Use the
   internal interface names.

+---------------+-----------------------------+---------------------------+
|               | External interface          | Internal interface        |
+---------------+-----------------------------+---------------------------+
| Login generic | login.hpc.uantwerpen.be     |                           |
|               | login-hopper.uantwerpen.be  |                           |
+---------------+-----------------------------+---------------------------+
| Login nodes   | login1-hopper.uantwerpen.be | ln01.hopper.antwerpen.vsc |
|               | login2-hopper.uantwerpen.be | ln02.hopper.antwerpen.vsc |
|               | login3-hopper.uantwerpen.be | ln03.hopper.antwerpen.vsc |
|               | login4-hopper.uantwerpen.be | ln04.hopper.antwerpen.vsc |
+---------------+-----------------------------+---------------------------+

Storage organisation
~~~~~~~~~~~~~~~~~~~~

The storage is organised according to the `VSC storage
guidelines <\%22/cluster-doc/access-data-transfer/where-store-data\%22>`__.

+-----------+-----------+-----------+-----------+-----------+-----------+
| Name      | Variable  | Type      | Access    | Backup    | Default   |
|           |           |           |           |           | quota     |
+===========+===========+===========+===========+===========+===========+
| /user/ant | $VSC_HOME | GPFS      | VSC       | NO        | 3 GB      |
| werpen/20 |           |           |           |           |           |
| X/vsc20XY |           |           |           |           |           |
| Z         |           |           |           |           |           |
+-----------+-----------+-----------+-----------+-----------+-----------+
| /data/ant | $VSC_DATA | GPFS      | VSC       | NO        | 25 GB     |
| werpen/20 |           |           |           |           |           |
| X/vsc20XY |           |           |           |           |           |
| Z         |           |           |           |           |           |
+-----------+-----------+-----------+-----------+-----------+-----------+
| /scratch/ | $VSC_SCRA | GPFS      | Hopper    | NO        | 25 GB     |
| antwerpen | TCH       |           | Leibniz   |           |           |
| /20X/vsc2 | $VSC_SCRA |           |           |           |           |
| 0XYZ      | TCH_SITE  |           |           |           |           |
+-----------+-----------+-----------+-----------+-----------+-----------+
| /small/an |           | GPFS      | Hopper    | NO        | 0 GB      |
| twerpen/2 |           |           | Leibniz   |           |           |
| 0X/vsc20X |           |           |           |           |           |
| YZ:sup:`( |           |           |           |           |           |
| *)`       |           |           |           |           |           |
+-----------+-----------+-----------+-----------+-----------+-----------+
| /tmp      | $VSC_SCRA | ext4      | Node      | NO        | 250 GB    |
|           | TCH_NODE  |           |           |           | hopper    |
+-----------+-----------+-----------+-----------+-----------+-----------+

:sup:`(*)` /small is a file system optimised for the storage of small
files of types that do not belong in $VSC_HOME. The file systems pointed
at by $VSC_DATA and $VSC_SCRATCH have a large fragment size (128 kB) for
optimal performance on larger files and since each file occupies at
least one fragment, small files waste a lot of space on those file
systems. The file system is available on request.

For users from other universities, the quota on $VSC_HOME and $VSC_DATA
will be determined by the local policy of your home institution as these
file systems are mounted from there. The pathnames will be similar with
trivial modifications based on your home institution and VSC account
number.

.. _characteristics-of-the-compute-nodes-1:

Characteristics of the compute nodes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Since hopper is currently a homogenous system with respect to CPU type
and interconnect, it is not needed to specify these properties (see also
the page on `specifying resources, output files and
notifications <\%22/cluster-doc/running-jobs/specifying-requirements\%22>`__).

However, to make it possible to write job scripts that can be used on
both hopper and turing (or other VSC clusters) and to prepare for future
extensions of the cluster, the following features are defined:

+-----------------------------------+-----------------------------------+
| property                          | explanation                       |
+===================================+===================================+
| ivybridge                         | only use Intel processors from    |
|                                   | the Ivy Bridge family (E5-XXXv2)  |
|                                   | (Not needed at the moment as this |
|                                   | is the only CPU type)             |
+-----------------------------------+-----------------------------------+
| ib                                | use InfiniBand interconnect (only |
|                                   | for compatibility with Turing job |
|                                   | scripts as all nodes have         |
|                                   | InfiniBand)                       |
+-----------------------------------+-----------------------------------+
| mem64                             | use nodes with 64 GB RAM (58 GB   |
|                                   | available)                        |
+-----------------------------------+-----------------------------------+
| mem256                            | use nodes with 256 GB RAM (250 GB |
|                                   | available)                        |
+-----------------------------------+-----------------------------------+

These characteristics map to the following nodes on Hopper:

+-------+-------+-------+-------+-------+-------+-------+-------+-------+
| Type  | CPU   | Inter | #     | #     | #     | insta | avail | local |
| of    | type  | conne | nodes | physi | logic | lled  | mem   | disc  |
| node  |       | ct    |       | cal   | al    | mem   | (per  |       |
|       |       |       |       | cores | cores | (per  | node) |       |
|       |       |       |       | (per  | (per  | node) |       |       |
|       |       |       |       | node) | node) |       |       |       |
+=======+=======+=======+=======+=======+=======+=======+=======+=======+
| ivybr | `Xeon | IB-FD | 144   | 20    | 20    | 64 GB | 56 GB | ~360  |
| idge: | E5-26 | R10   |       |       |       |       |       | GB    |
| ib:me | 80v2  |       |       |       |       |       |       |       |
| m64   | <\%22 |       |       |       |       |       |       |       |
|       | https |       |       |       |       |       |       |       |
|       | ://ar |       |       |       |       |       |       |       |
|       | k.int |       |       |       |       |       |       |       |
|       | el.co |       |       |       |       |       |       |       |
|       | m/pro |       |       |       |       |       |       |       |
|       | ducts |       |       |       |       |       |       |       |
|       | /7527 |       |       |       |       |       |       |       |
|       | 7\%22 |       |       |       |       |       |       |       |
|       | >`__  |       |       |       |       |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+-------+
| ivybr | `Xeon | IB-FD | 24    | 20    | 20    | 256   | 248   | ~360  |
| idge: | E5-26 | R10   |       |       |       | GB    | GB    | GB    |
| ib:me | 80v2  |       |       |       |       |       |       |       |
| m256  | <\%22 |       |       |       |       |       |       |       |
|       | https |       |       |       |       |       |       |       |
|       | ://ar |       |       |       |       |       |       |       |
|       | k.int |       |       |       |       |       |       |       |
|       | el.co |       |       |       |       |       |       |       |
|       | m/pro |       |       |       |       |       |       |       |
|       | ducts |       |       |       |       |       |       |       |
|       | /7527 |       |       |       |       |       |       |       |
|       | 7\%22 |       |       |       |       |       |       |       |
|       | >`__  |       |       |       |       |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+-------+

Turing
------

In July 2009, the UAntwerpen bought a 768 core cluster (L5420 CPUs, 16
GB RAM/node) from HP, that was installed and configured in December
2009. In December 2010, the cluster was extended with 768 cores (L5640
CPUs, 24 GB RAM/node). In September 2011, another 96 cores (L5640 CPUs,
24 GB RAM/node) have been added. Turing has been retired in January
2017.

"
