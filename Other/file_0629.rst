High performance and multiscale computing: blood, clay, stars and humans
------------------------------------------------------------------------

*Speaker: Derek Groen (Centre for Computational Science, University
College London)*

Multiscale simulations are becoming essential across many scientific
disciplines. The concept of having multiple models form a single
scientific simulation, with each model operating on its own space and
time scale, gives rise to a range of new challenges and trade-offs. In
this talk, I will present my experiences with high performance and
multiscale computing. I have used supercomputers for modelling
clay-polymer nanocomposites [1], blood flow in the human brain [2], and
dark matter structure formation in the early universe [3]. I will
highlight some of the scientific advances we made, and present the
technologies we developed and used to enable simulations across
supercomputers (using multiple models where convenient). In addition, I
will reflect on the non-negligible aspect of human effort and policy
constraints, and share my experiences in enabling challenging
calculations, and speeding up more straightforward ones.

[`slides - PDF 8.3MB <\%22/assets/1057\%22>`__]

References
~~~~~~~~~~

#. James L. Suter, Derek Groen, and Peter V. Coveney. `Chemically
   Specific Multiscale Modeling of Clay–Polymer Nanocomposites Reveals
   Intercalation Dynamics, Tactoid Self-Assembly and Emergent Materials
   Properties <\%22http://dx.doi.org/10.1002/adma.201403361\%22>`__.
   Advanced Materials, volume 27, issue 6, pages 966–984. (DOI:
   `10.1002/adma.201403361 <\%22http://dx.doi.org/10.1002/adma.201403361\%22>`__)
#. Mohamed A. Itani, Ulf D. Schiller, Sebastian Schmieschek, James
   Hetherington, Miguel O. Bernabeu, Hoskote Chandrashekar, Fergus
   Robertson, Peter V. Coveney, and Derek Groen. `An automated
   multiscale ensemble simulation approach for vascular blood
   flow <\%22http://dx.doi.org/10.1016/j.jocs.2015.04.008\%22>`__.
   Journal of Computational Science, volume 9, pages 150-155. (DOI:
   `10.1016/j.jocs.2015.04.008 <\%22http://dx.doi.org/10.1016/j.jocs.2015.04.008\%22>`__)
#. Derek Groen and Simon Portugies Zwart. `From Thread to
   Transcontinental Computer: Disturbing Lessons in Distributed
   Supercomputing <\%22http://dx.doi.org/10.1109/eScience.2015.81>`__.
   2015 IEEE 11th International Conference on e-Science, IEEE, pages
   565-571. (DOI:
   `10.1109/eScience.2015.81 <\%22http://dx.doi.org/10.1109/eScience.2015.81>`__)

High-performance computing of wind farms in the atmospheric boundary layer
--------------------------------------------------------------------------

*Speaker: Johan Meyers (Department of Mechanical Engineering, KU
Leuven)*

The aerodynamics of large wind farms are governed by the interaction
between turbine wakes, and by the interaction of the wind farm as a
whole with the atmospheric boundary layer. The deceleration of the flow
in the farm that is induced by this interaction, leads to an efficiency
loss for wind turbines downstream in the farm that can amount up to 40%
and more. Research into a better understanding of wind-farm boundary
layer interaction is an important driver for reducing this efficiency
loss. The physics of the problem involves a wide range of scales, from
farm scale and ABL scale (requiring domains of several kilometers cubed)
down to turbine and turbine blade scale with flow phenomena that take
place on millimeter scale. Modelling such a system, requires a
multi-scale approach in combination with extensive supercomputing. To
this end, our simulation code SP-Wind is used. Implementation issues and
parallelization are discussed. Next to that, new physical insights
gained from our simulations at the VSC are highlighted.

[`slides - PDF 9.9MB <\%22/assets/1055\%22>`__]

"
