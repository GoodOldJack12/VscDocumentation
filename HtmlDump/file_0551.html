<h2>What are toolchains?</h2><p>A toolchain is a collection of tools to build (HPC) software consistently. It consists of
</p>
<ul>
	<li>compilers for C/C++ and Fortran,</li>
	<li>a communications library (MPI), and</li>
	<li>mathematical libraries (linear algebra, FFT).</li>
</ul>
<p>Toolchains are versioned, and refreshed twice a year. All software available on the cluster is rebuild when a new version of a toolchain is defined to ensure consistency. Version numbers consist of the year of their definition, followed by either <code>a</code>
or <code>b</code>, e.g.,
<code>2014a</code>. 
Note that the software components are not necessarily the most recent releases, rather they are selected for stability and reliability.
</p>
<p>Two toolchain flavors are standard across the VSC on all machines that can support them: <code>intel</code> (based on Intel software components) and 
<code>foss</code> (based on free and open source software).
</p>
<p>It may be of interest to note that the Intel C/C++ compilers are more strict with respect to the standards than the GCC C/C++ compilers, while for Fortran, the GCC Fortran compiler tracks the standard more closely, while Intel's Fortran allows for many extensions added during Fortran's long history. When developing code, one should always build with both compiler suites, and eliminate all warnings.
</p>
<p>On average, the Intel compiler suite produces executables that are 5 to 10 % faster than those generated using the GCC compiler suite. However, for individual applications the differences may be more significant with sometimes significantly faster code produced by the Intel compilers while on other applications the GNU compiler may produce much faster code.
</p>
<p>Additional toolchains may be defined on specialised hardware to extract the maximum performance from that hardware.
</p>
<ul>
	<li>On Cerebro, the SGI UV shared memory system at the KU Leuven, you need to use the SGI MPI-library (called MPT for Message Passing Toolkit) to get the maximum performance from the interconnect (which offers hardware acceleration for some MPI functions). On that machine, two additional toolchains are defined, <code>intel-mpt</code> and 
	<code>foss-mpt</code>, equivalent to the standard 
	<code>intel</code> and 
	<code>foss</code> 
	toolchains respectively but with the MPI library replaced with MPT.</li>
</ul>
<h2><a name=\"intel-toolchain\">Intel toolchain</a></h2>
<p>The <code>intel</code> toolchain consists almost entirely of software components 
developed by Intel. When building third-party software, or developing your own, 
load the module for the toolchain:
</p>
<pre>$ module load intel/&lt;version&gt;
</pre>
<p>where <code>&lt;version&gt;</code> should be replaced by the one to be used, e.g., 
<code<2014a</code>. See the documentation on the software module system for more details.
</p>
<p>Starting with the <code>2014b</code> toolchain, the GNU compilers are also included in 
this toolchain as the Intel compilers use some of the libraries and as it is possible 
(though some care is needed) to link code generated with the Intel compilers with code 
compiled with the GNU compilers.
</p>
<h3><a name=\"intel-compilers\">Compilers: Intel and Gnu</a></h3>
<p>Three compilers are available:
</p>
<ul>
	<li>C: <code>icc</code></li>
	<li>C++: <code>icpc</code></li>
	<li>Fortran: <code>ifort</code></li>
</ul>
<p>Recent versions of
</p>
<p>For example, to compile/link a Fortran program <code>fluid.f90</code> to an executable 
<code>fluid</code> with architecture specific optimization, use:
</p>
<pre>$ ifort  -O2  -xhost  -o fluid  fluid.f90
</pre>
<p>Documentation on Intel compiler flags and options is 
<a href=\"https://software.intel.com/sites/default/files/Compiler_QRG_2013.pdf\">provided 
by Intel</a>. Do not forget to <em>load the toolchain module</em> first!
</p>
<h4><a name=\"intel-openmp\"></a>Intel OpenMP</h4>
<p>The compiler switch to use to compile/link OpenMP C/C++ or Fortran code is 
<code>-openmp</code>.  For example, to compile/link a OpenMP C program 
<code>scatter.c</code> to an executable 
<code>scatter</code> with architecture specific 
optimization, use:
</p>
<pre>$ icc  -openmp  -O2  -xhost  -o scatter  scatter.c
</pre>
<p>Remember to specify as many processes per node as the number of threads the executable 
is supposed to run. This can be done using the <code>ppn</code> resource, e.g., 
<code>-l nodes=1:ppn=10</code> for an executable that should be run with 10 OpenMP 
threads. The number of threads should not exceed the number of cores on a compute node.
</p>
<h3><a name=\"intel-mpi\">Communication library: Intel MPI</a></h3>
<p>For the intel toolchain, <code>impi</code>, i.e., Intel MPI is used as the 
communications library. To compile/link MPI programs, wrappers are supplied, so that 
the correct headers and libraries are used automatically. These wrappers are:
</p>
<ul>
	<li>C: <code>mpiicc</code></li>
	<li>C++: <code>mpiicpc</code></li>
	<li>Fortran: <code>mpiifort</code></li>
</ul>
<p>Note that the <em>names differ</em> from those of other MPI implementations. 
The compiler wrappers take the same options as the corresponding compilers.
</p>
<h4>Using the Intel MPI compilers</h4>
<p>For example, to compile/link a C program <code>thermo.c</code> to an executable 
<code>thermodynamics</code> with architecture specific optimization, use:
</p>
<pre>$ mpiicc -O2  -xhost  -o thermodynamics  thermo.c
</pre>
<p>Extensive documentation is 
<a href=\"https://software.intel.com/en-us/articles/intel-mpi-library-documentation\">provided 
by Intel</a>. Do not forget to <em>load the toolchain module</em> first.
</p>
<h4>Running an MPI program with Intel MPI</h4>
<p>Note that an MPI program must be run with the exact same version of the toolchain as 
it was originally build with. The listing below shows a PBS job script 
<code>thermodynamics.pbs</code> that runs the 
<code>thermodynamics</code> executable.
</p>
<pre>#!/bin/bash -l
module load intel/&lt;version&gt;
cd $PBS_O_WORKDIR n_proc=$( cat $PBS_NODEFILE  |  wc  -l )
mpirun  -np $n_proc  ./thermodynamics
</pre>
<p>The number of processes is computed from the length of the node list in the 
<code>$PBS_NODEFILE</code> file, which in turn is specified as a resource specification 
when submitting the job to the queue system.
</p>
<h3><a name=\"intel-mkl\">Intel mathematical libraries</a></h3>
<p>The Intel Math Kernel Library (MKL) is a comprehensive collection of highly optimized 
libraries that form the core of many scientific HPC codes. Among other functionality, 
it offers:
</p>
<ul>
	<li>BLAS (Basic Linear Algebra Subsystem), and extensions to sparse matrices</li>
	<li>Lapack (Linear algebra package) and ScaLAPACK (the distributed memory version)</li>
	<li>FFT-routines including routines compatible with the FFTW2 and FFTW3 libraries 
	(Fastest Fourier Transform in the West)</li>
	<li>Various vector functions and statistical functions that are optimised for the 
	vector instruction sets of all recent Intel processor families</li>
</ul>
<p>Intel offers 
<a href=\"https://software.intel.com/en-us/articles/intel-math-kernel-library-documentation\">extensive 
documentation</a> on this library and how to use it.
</p>
<p>There are two ways to link the MKL library:
</p>
<ul>
	<li>If you use icc, icpc or ifort to link your code, you can use the -mkl compiler 
	option:
	<ul>
		<li>-mkl=parallel or -mkl: Link the multi-threaded version of the library.</li>
		<li>-mkl=sequential: Link the single-threaded version of the library</li>
		<li>-mkl=cluster: Link the cluster-specific and sequential library, i.e., 
		ScaLAPACK will be included, but assumes one process per core (so no hybrid MPI/multi-threaded approach)</li>
	</ul>
	The Fortran95 interface library for lapack is not automatically included though. 
	You'll have to specify that library seperately. You can get the value from the 
	<a href=\"https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor\">MKL 
	Link Line Advisor</a>, see also the next item.</li>
	<li>Or you can specify all libraries explictly. To do this, it is strongly recommended 
	to use Intel's 
	<a href=\"https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor/\">MKL 
	Link Line Advisor</a>, and will also tell you how to link the MKL library with 
	code generated with the GNU and PGI compilers.<br>
	<strong>Note:</strong> On most VSC systems, the variable MKLROOT has a different 
	value from the one assumed in the Intel documentation. Wherever you see 
	<code>$(MKLROOT)</code> you may have to replace it with 
	<code>$(MKLROOT)/mkl</code>.</li>
</ul>
<p>MKL also offers a very fast streaming pseudorandom number generator, see the 
documentation for details.
</p>
<h3><a name=\"intel-versions\"></a>Intel toolchain version numbers</h3>
<table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" style=\"width: 100%;\">
<tbody>
<tr>
	<th>
	</th>
	<th>2014a
	</th>
	<th>2014b
	</th>
	<th>2015a
	</th>
</tr>
<tr>
	<td>icc
	</td>
	<td>13.1.3 20130607
	</td>
	<td>13.1.3 20130607
	</td>
	<td>15.0.1 20141023
	</td>
</tr>
<tr>
	<td>icpc
	</td>
	<td>13.1.3 20130607
	</td>
	<td>13.1.3 20130607
	</td>
	<td>15.0.1 20141023
	</td>
</tr>
<tr>
	<td>ifort
	</td>
	<td>13.1.3 20130607
	</td>
	<td>13.1.3 20130607
	</td>
	<td>15.0.1 20141023
	</td>
</tr>
<tr>
	<td>Intel MPI
	</td>
	<td>4.1.3.045
	</td>
	<td>4.1.3.049
	</td>
	<td>5.0.2.044
	</td>
</tr>
<tr>
	<td>Intel MKL
	</td>
	<td>11.1.1.106
	</td>
	<td>11.1.2.144
	</td>
	<td>11.2.1.133
	</td>
</tr>
<tr>
	<td>GCC
	</td>
	<td>/
	</td>
	<td>4.8.3
	</td>
	<td>4.9.2
	</td>
</tr>
</tbody>
</table>
<h3><a name=\"intelInfo\"></a>Further information on Intel tools</h3>
<ul>
	<li>All Intel documentation of recent software versions is available in the 
	<a href=\"https://software.intel.com/en-us/intel-software-technical-documentation\">Intel 
	Software Documentation Library</a>. The documentation is typically available for the most recent version and sometimes one older version of te compiler and libraries.</li>
	<li>MKL
	<ul>
		<li><a href=\"https://software.intel.com/en-us/articles/intel-math-kernel-library-documentation/\">Link 
		page to the documentation of the most recent version on the Intel web site</a></li>
		<li><a href=\"https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor\">MKL 
		Link Line Advisor</a></li>
		<li>Older versions:
		<ul>
			<li>intel/2014a and intel/2014b toolchain, MKL 11.1: 
			<a href=\"https://software.intel.com/en-us/mkl_11.1_ug_lin\">User's Guide</a> 
			and <a href=\"https://software.intel.com/en-us/mkl_11.1_ref\">Reference Guide</a></li>
		</ul>
		</li>
	</ul>
	</li>
	<li><a href=\"/cluster-doc/development/blas-lapack#Links\">Generic BLAS/LAPACK/ScaLAPACK 
	documentation</a></li>
</ul>
<h2><a name=\"foss-toolchain\">FOSS toolchain</a></h2>
<p>The <code>foss</code> toolchain consists entirely of free and open source software 
components. When building third-party software, or developing your own, 
load the module for the toolchain:
</p>
<pre>$ module load foss/&lt;version&gt;
</pre>
<p>where <code>&lt;version&gt;</code> should be replaced by the one to be used, e.g., 
<code>2014a</code>. See the documentation on the software module system for more details.
</p>
<h3><a name=\"gcc-compilers\">Compilers: GNU</a></h3>
<p>Three GCC compilers are available:
</p>
<ul>
	<li>C: <code>gcc</code></li>
	<li>C++: <code>g++</code></li>
	<li>Fortran: <code>gfortran</code></li>
</ul>
<p>For example, to compile/link a Fortran program <code>fluid.f90</code> to an executable 
<code>fluid</code> with architecture specific optimization for processors that support AVX instructions, use:
</p>
<pre>$ gfortran -O2 -march=corei7-avx -o fluid fluid.f90
</pre>
<p>Documentation on GCC compiler flags and options is available on the 
<a href=\"http://gcc.gnu.org/onlinedocs/\">project's website</a>. Do not forget to load the 
toolchain module first!
</p>
<h4><a name=\"foss-openmp\"></a>GCC OpenMP</h4>
<p>The compiler switch to use to compile/link OpenMP C/C++ or Fortran code is 
<code>-fopenmp</code>. For example, to compile/link a OpenMP C program 
<code>scattter.c</code> to an executable 
<code>scatter</code> with optimization for processors that support the AVX instruction 
set, use:
</p>
<pre>$ gcc -fopenmp -O2 -march=corei7-avx -o scatter scatter.c
</pre>
<p>Remember to specify as many processes per node as the number of threads the 
executable is supposed to run. This can be done using the <code>ppn</code> resource, e.g., 
<code>-l nodes=1:ppn=10</code> for an executable that should be run with 10 OpenMP threads. 
The number of threads should not exceed the number of cores on a compute node.
</p>
<p>Note that the OpenMP runtime library used by GCC is of inferior quality when compared 
to Intel's, so developers are strongly encouraged to use the 
<a href=\"#intel-toolchain\"><code>intel</code> toolchain</a> when developing/building OpenMP software.
</p>
<h3><a name=\"openmpi\">Communication library: OpenMPI</a></h3>
<p>For the <code>foss</code> toolchain, OpenMPI is used as the communications library. 
To compile/link MPI programs, wrappers are supplied, so that the correct headers and 
libraries are used automatically. These wrappers are:
</p>
<ul>
	<li>C: <code>mpicc</code></li>
	<li>C++: <code>mpic++</code></li>
	<li>Fortran: <code>mpif77</code>, 
	<code>mpif90</code></li>
</ul>
<p>The compiler wrappers take the same options as the corresponding compilers.
</p>
<h4>Using the MPI compilers from OpenMPI</h4>
<p>For example, to compile/link a C program <code>thermo.c</code> to an executable 
<code>thermodynamics</code> with architecture specific optimization for the AVX 
instruction set, use:
</p>
<pre>$ mpicc -O2 -march=corei7-avx -o thermodynamics thermo.c
</pre>
<p>Extensive documentation is <a href=\"http://www.open-mpi.org/doc/\">provided on the 
project's website</a>. Do not forget to load the toolchain module first.
</p>
<h4>Running an OpenMPI program</h4>
<p>Note that an MPI program must be run with the exact same version of the toolchain as 
it was originally build with. The listing below shows a PBS job script 
<code>thermodynamics.pbs</code> that runs the 
<code>thermodynamics</code> executable.
</p>
<pre>#!/bin/bash -l 
module load intel/&lt;version&gt; 
cd $PBS_O_WORKDIR 
mpirun ./thermodynamics
</pre>
<p>The hosts and number of processes is retrieved from the queue system, that gets this 
information from the resource specification for that job.
</p>
<h3><a name=\"foss-mathlibs\">FOSS mathematical libraries</a></h3>
<p>The foss toolchain contains the basic HPC mathematical libraries, it offers:
</p>
<ul>
	<li><a href=\"http://www.openblas.net/\">OpenBLAS</a> (Basic Linear Algebra Subsystem)</li>
	<li><a href=\"http://www.netlib.org/lapack/\">Lapack </a>(Linear Algebra PACKage)</li>
	<li>ScaLAPACK (Scalable Linear Algebra PACKage)</li>
	<li><a href=\"http://www.fftw.org/\">FFTW</a> (Fastest Fourier Transform in the West)</li>
</ul>
<h3><a name=\"foss-versions\"></a>Version numbers FOSS toolchain</h3>
<table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" style=\"width: 100%;\">
<tbody>
<tr>
	<th>
	</th>
	<th>2014a
	</th>
	<th>2014b
	</th>
	<th>2015a
	</th>
</tr>
<tr>
	<td>GCC
	</td>
	<td>4.8.2
	</td>
	<td>4.8.3
	</td>
	<td>4.9.2
	</td>
</tr>
<tr>
	<td>OpenMPI
	</td>
	<td>1.6.5
	</td>
	<td>1.8.1
	</td>
	<td>1.8.3
	</td>
</tr>
<tr>
	<td>OpenBLAS
	</td>
	<td>0.2.8
	</td>
	<td>0.2.9
	</td>
	<td>0.2.13
	</td>
</tr>
<tr>
	<td>LAPACK
	</td>
	<td>3.5.0
	</td>
	<td>3.5.0
	</td>
	<td>3.5.0
	</td>
</tr>
<tr>
	<td>ScaLAPACK
	</td>
	<td>2.0.2
	</td>
	<td>2.0.2
	</td>
	<td>2.0.2
	</td>
</tr>
<tr>
	<td>FFTW
	</td>
	<td>3.3.3
	</td>
	<td>3.3.4
	</td>
	<td>3.3.4
	</td>
</tr>
</tbody>
</table>
<h3><a name=\"fossInfo\"></a>Further information on FOSS components</h3>
<ul>
	<li><a href=\"https://gcc.gnu.org/onlinedocs/\">Overview of GCC manuals 
	(all versions)</a></li>
	<li>OpenMPI documentation
	<ul>
		<li><a href=\"http://www.open-mpi.org/doc/v1.8/\">1.8.x (foss/2014b and foss/2015a)</a></li>
		<li><a href=\"http://www.open-mpi.org/doc/v1.6/\">1.6.x (foss/2014a)</a></li>
	</ul>
	</li>
	<li>The <a href=\"http://www.openblas.net/\">OpenBLAS project page</a> and 
	<a href=\"https://github.com/xianyi/OpenBLAS/wiki\">documentation Wiki</a></li>
	<li><a href=\"/cluster-doc/development/blas-lapack#Links\">Generic BLAS/LAPACK/ScaLAPACK 
	documentation</a></li>
</ul>"

