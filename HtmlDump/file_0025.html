<p>The successor of Muk is expected to be installed in the spring 2016.</p><p>There is also a small test cluster for experiments with accellerators (GPU and Intel Xeon Phi) with a view to using this technology in future VSC clusters.</p><h2>The Tier-1 cluster Muk</h2><p>The Tier-1 cluster Muk has 528 computing nodes, each with two 8-core Intel Xeon processors from the Sandy Bridge generation (E5-2670, 2.6 GHz). Each node features 64 GiB RAM, for a total memory capacity of more than 33 TiB. The computing nodes are connected by an FDR InfiniBand interconnect with a fat tree topology. This network has a high bandwidth (more than 6,5GB / s per direction per link) and a low latency. The storage is provided by a disk system with a total disk capacity of 400 TB and a peak bandwidth of 9.5 GB / s.</p><p>The cluster achieves a peak performance of more than 175 Tflops and a Linpack performance of 152.3 Tflops. With this result, the cluster was for 5 consecutive periods in the Top500 list of fastest supercomputers in the world:</p><table> <tbody><tr> <td> <p>List</p> </td> <td> <p>06/2012</p> </td> <td> <p>11/2012</p> </td> <td> <p>06/2013</p> </td> <td> <p>11/2013</p> </td> <td> <p>06/2014</p> </td> </tr> <tr> <td> <p>Position</p> </td> <td> <p>118</p> </td> <td> <p>163</p> </td> <td> <p>239</p> </td> <td> <p>306</p> </td> <td> <p>430</p> </td> </tr> </tbody></table><p>In November 2014 the cluster fell just outside the list but still took 99% of the performance of the system in place 500.</p><h2>Accellerator testbed</h2><p>In addition to the tier-1 cluster Muk, the VSC has an experimental GPU / Xeon Phi cluster. 8 nodes in this cluster have 2 K20x nVidia GPUs with accompanying software stack, and 8 nodes are equipped with two Intel Xeon Phi 5110P (\"Knight's Corner\" generation) boards. The nodes are interconnected by means of a QDR InfiniBand network. For practical reasons, these nodes were integrated into the KU Leuven / Hasselt University Tier-2 infrastructure.</p><h2>Software</h2><p>Like on all other VSC-clusters, the operating system of Muk is a variant of Linux, in this case Scientific Linux which in turn based on Red Hat Linux. The system also features a comprehensive stack of software development tools which includes the GNU and Intel compilers, debugger and profiler for parallel applications and different versions of OpenMPI and Intel MPI.</p><p>There is also an extensive set of freely available applications installed on the system. More software can be installed at the request of the user. Users however have to take care of the software licenses when the software is not freely available, and therefore also for the financing of that license.</p><p><a href=\"/cluster-doc/software/tier1-muk\">Detailed overview of the installed software</a></p><h2>Access to the Tier-1 system</h2><p>Academic users can access the Tier-1 cluster Muk through a project application. There are two types of project applications</p><ul><li>The Tier-1 starting grant of up to 100 node days to test and / or to optimize software, typically with a view to a regular request for computing time. There is a continuous assessment process for this project type.<br><a href=\"/en/access-and-infrastructure/tier1-starting-grant\">Learn more</a></li> <li>The regular project application, for allocations between 500 and 5000 node days. The applications are assessed on scientific excellence and technical feasibility by an evaluation committee of foreign experts. There are three cut-off dates a year at which the submitted project proposals are evaluated. The users are also expected to pay a small contribution towards the cost.<br><a href=\"/en/access-and-infrastructure/project-access-tier1\">Learn more</a></li></ul><p>To use the GPU / Xeon Phi cluster it is sufficient to contact the <a href=\"/en/about-vsc/contact\">HPC coordinator of your institution</a>.</p><p>Industrial users and non-Flemish research institutions and not-for-profit organizations can also <a href=\"/en/access-and-infrastructure/access-industry\">purchase computing time on the Tier-1 Infrastructure</a>. For this you can contact the <a href=\"/en/about-vsc/contact\">Hercules Foundation</a>.</p>
