<h2><a name=\"DerekGroen\"></a>High performance and multiscale computing: blood, clay, stars and humans</h2><p><em>Speaker: Derek Groen (Centre for Computational Science, University College London)</em></p><p>Multiscale simulations are becoming essential across many scientific disciplines. The concept of having multiple models form a single scientific simulation, with each model operating on its own space and time scale, gives rise to a range of new challenges and trade-offs. In this talk, I will present my experiences with high performance and multiscale computing. I have used supercomputers for modelling clay-polymer nanocomposites [1], blood flow in the human brain [2], and dark matter structure formation in the early universe [3]. I will highlight some of the scientific advances we made, and present the technologies we developed and used to enable simulations across supercomputers (using multiple models where convenient). In addition, I will reflect on the non-negligible aspect of human effort and policy constraints, and share my experiences in enabling challenging calculations, and speeding up more straightforward ones.
</p><p>[<a href=\"/assets/1057\">slides - PDF 8.3MB</a>]</p><h3>References</h3><ol class=\"list--ordered\">
	<li>James L. Suter, Derek Groen, and Peter V. Coveney. <a href=\"http://dx.doi.org/10.1002/adma.201403361\">Chemically Specific Multiscale Modeling of Clay–Polymer Nanocomposites Reveals Intercalation Dynamics, Tactoid Self-Assembly and Emergent Materials Properties</a>. Advanced Materials, volume 27, issue 6, pages 966–984. (DOI: <a href=\"http://dx.doi.org/10.1002/adma.201403361\">10.1002/adma.201403361</a>)</li>
	<li>Mohamed A. Itani, Ulf D. Schiller, Sebastian Schmieschek, James Hetherington, Miguel O. Bernabeu, Hoskote Chandrashekar, Fergus Robertson, Peter V. Coveney, and Derek Groen. <a href=\"http://dx.doi.org/10.1016/j.jocs.2015.04.008\">An automated multiscale ensemble simulation approach for vascular blood flow</a>. Journal of Computational Science, volume 9, pages 150-155. (DOI: <a href=\"http://dx.doi.org/10.1016/j.jocs.2015.04.008\">10.1016/j.jocs.2015.04.008</a>)</li>
	<li>Derek Groen and Simon Portugies Zwart. <a href=\"http://dx.doi.org/10.1109/eScience.2015.81 \">From Thread to Transcontinental Computer: Disturbing Lessons in Distributed Supercomputing</a>. 2015 IEEE 11th International Conference on e-Science, IEEE, pages 565-571. (DOI: <a href=\"http://dx.doi.org/10.1109/eScience.2015.81 \">10.1109/eScience.2015.81</a>)
	</li>
</ol><h2><a name=\"JohanMeyers\"></a>High-performance computing of wind farms in the atmospheric boundary layer</h2><p><em>Speaker: Johan Meyers (Department of Mechanical Engineering, KU Leuven)</em>
</p><p>
The aerodynamics of large wind farms are governed by the interaction between turbine wakes, and by the interaction of the wind farm as a whole with the atmospheric boundary layer. The deceleration of the flow in the farm that is induced by this interaction, leads to an efficiency loss for wind turbines downstream in the farm that can amount up to 40% and more. Research into a better understanding of wind-farm boundary layer interaction is an important driver for reducing this efficiency loss. The physics of the problem involves a wide range of scales, from farm scale and ABL scale (requiring domains of several kilometers cubed) down to turbine and turbine blade scale with flow phenomena that take place on millimeter scale. Modelling such a system, requires a multi-scale approach in combination with extensive supercomputing. To this end, our simulation code SP-Wind is used. Implementation issues and parallelization are discussed. Next to that, new physical insights gained from our simulations at the VSC are highlighted.
</p><p>[<a href=\"/assets/1055\">slides - PDF 9.9MB</a>]</p>"

