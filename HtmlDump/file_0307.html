<h2>Hardware details</h2><ul>
	<li>The cluster <strong>login nodes</strong>:
	<ul>
		<li>login.hpc.kuleuven.be and login2.hpc.kuleuven.be (use this hostname if you read <em>vsc.login.node</em> in the documentation and want to connect to this login node).</li>
		<li>two GUI login nodes through NX server.</li>
	</ul>
	</li>
	<li><strong>Compute nodes</strong>:
	<ul>
		<li>
		<strong>Thin node section</strong>:
		<ul>
			<li>
			208 nodes with two 10-core \"Ivy Bridge\" Xeon E5-2680v2 CPUs (2.8 GHz, 25 MB level 3 cache). 176 of those nodes have 64 GB RAM while 32 are equiped with 128 GB RAM. The nodes are linked to a QDR Infiniband network. All nodes have a small local disk, mostly for swapping and the OS image.
			</li>
			<li>
			144 nodes with two 12-core \"Haswell\" Xeon E5-2680v3 CPUs (2.5 GHz, 30 MB level 3 cache). 48 of those nodes have with 64 GB RAM while 96 are equiped with 128 GB RAM. These nodes are linked to a FDR Infiniband network which offers lower latency and higher bandwidth than QDR.
			</li>
		</ul>
		The total memory capacity of this section is 30 TB, the total peak performance is about 232 Tflops in double precision arithmetic.
		</li>
		<li>
		<strong><a id=\"Cerebro\" name=\"Cerebro\"></a>SMP section</strong> (also known as Cerebro): a SGI UV2000 system with 64 sockets with a 10-core \"Ivy Bridge\" Xeon E5-4650 CPU (2.4 GHz, 25 MB level 3 cache), spread over 32 blades and connected through a SGI-proprietary NUMAlink6 interconnect. The interconnect also offers support for global address spaces across shared memory partitions and offload of some MPI functions. 16 sockets have 128 GB RAM and 48 sockets have 256 GB RAM, for a total RAM capacity of 14 TB. The peak compute performance is 12.3 Tflops in double precision arithmetic. The SMP system also contains a fast 21.8 GB disk storage system for swapping and temporary files.
        The system is partitioned in 2 shared memory partitions. 1 partition has 480 cores and 12 TB and 1 partition with 160 cores and 2TB. Both partitions have 10TB local scratch space.
		<br>
		However, should the need arise it can be reconfigured into a single large 64-socket shared memory machine.More information can be found in the  <a target=\"_blank\" href=\"https://www.vscentrum.be/assets/965\">cerebro quick start guide</a> or <a target=\"_blank\" href=\"https://www.vscentrum.be/assets/947\">slides from the info-session.</a></li>
		<li><strong>Accelerator section:</strong>
		<ul>
			<li>5 nodes with two 10-core \"Haswell\" Xeon E5-2650v3 2.3GHz CPUs, 64 GB of RAM 
and 2 GPUs Tesla K40 (2880 CUDA cores @ Boost clocks 810 MHz and 875 MHz, 1.66 DP Tflops/GPU Boost Clocks).
			</li>
		</ul>
		<ul>
			<li> The central GPU and Xeon Phi system is also integrated in the cluster and available to other sites. Each node has two six-core Intel Xeon E5-2630 CPUs, 64 GB RAM and a
 local hard disk. All nodes are on a QDR Infiniband interconnect. This system consists of:
			</li>
			<li>8 nodes have two nVidia K20x cards each installed. Each K20x has 14
 SMX processors (Kepler family; total of 2688 CUDA cores) that run at 
732MHz and 6 GB of GDDR5 memory with a peak memory bandwidth of 250 GB/s
 (384-bit interface @ 2.6 GHz). The peak floating point performance per 
card is 1.31 Tflops in double and 3.95 Tflops in single precision.
			</li>
			<li>8 nodes have two Intel Xeon Phi 5110P cards each installed. Each 
Xeon Phi board has 60 cores running at 1.053 GHz (of which one is 
reserved for the card OS and 59 are available for applications). Each 
core supports a large subset of the 64-bit Intel architecture 
instructions and a vector extension with 512-bit vector instructions. 
Each board contains 8 GB of RAM, distributed across 16 memory channels,
 with a peak memory bandwidth of 320 GB/s. The peak performance (not 
counting the core reserved for the OS) is 0.994 Tflops in double 
precision and 1.988 Tflops in single precision. The Xeon Phi system is not yet fully operational. MPI applications spanning multiple nodes cannot be used at the moment.
			</li>
			<li>20 nodes have four Nvidia Tesla P100 SXM2 cards each installed (3584 CUDA cores @1328 MHz, 5.3 DP Tflops/GPU).</li>
			<li>To start working with accelerators please refer to <a href=\"https://www.vscentrum.be/infrastructure/hardware/hardware-kul/accelerators\">access webpage</a>.</li>
		</ul>
		<ul>
		</ul></li>
	</ul>
	</li>
	<li><strong>Visualization nodes</strong>: 2 nodes with two 10-core \"Haswell\" Xeon E5-2650v3 2.3GHz CPUs, 2 times 64 GB of RAM 
and 2 GPUs NVIDIA
Quadro
K5200 (2304 CUDA cores @ 667 MHz). To start working on visualization nodes, we refer to the 
	<a target=\"_blank\" href=\"/client/multiplatform/turbovnc\">TurboVNC start guide</a>.</li>
	<li><strong>Central storage</strong> available to all nodes:
	<ul>
		<li>A NetApp NAS system with 30 TB of storage, used for the home- and permanent data directories. All data is mirrored almost instantaneously to the KU Leuven disaster recovery data centre.</li>
		<li>A 284 TB GPFS parallel filesystem from DDN, mostly used for temporary disk space.</li>
		<li>A 600 TB archive storage  optimised for capacity and aimed at long-term storage of very infrequently accessed data. To start using the archive storage, we refer to the 
		<a target=\"_blank\" href=\"https://www.vscentrum.be/infrastructure/hardware/wos-storage\">WOS Storage quick start guide</a>.</li>
	</ul>
	<ul>
	</ul>
	</li>
	<li>For administrative purposes, there are also <strong>service nodes</strong> that are not user-accessible</li>
</ul>
<p style=\"text-align: center;\"><img src=\"/assets/1335\" style=\"width: 1119px; height: 460px;\" width=\"1119\" height=\"560\">
</p>
<h2>Characteristics of the compute nodes</h2>
<p>The following properties allow you to select the appropriate node type for your job (see also the page on <a href=\"/cluster-doc/running-jobs/specifying-requirements\">specifying resources, output files and notifications</a>):
</p>
<table class=\"plain\">
<tbody>
<tr>
	<th>Cluster
	</th>
	<th>Type of node
	</th>
	<th>CPU type
	</th>
	<th>Interconnect
	</th>
	<th># cores
	</th>
	<th>installed mem
	</th>
	<th>avail mem
	</th>
	<th>local discs
	</th>
	<th># nodes
	</th>
</tr>
<tr>
	<td style=\"vertical-align: top;\">Thinking
	</td>
	<td style=\"vertical-align: top;\">ivybridge
	</td>
	<td style=\"vertical-align: top;\">Xeon E5-2680v2
	</td>
	<td style=\"vertical-align: top;\">IB-QDR
	</td>
	<td style=\"vertical-align: top;\">20
	</td>
	<td style=\"vertical-align: top;\">64 GB
	</td>
	<td style=\"vertical-align: top;\">60 GB
	</td>
	<td style=\"vertical-align: top;\">250 GB
	</td>
	<td style=\"vertical-align: top;\">176
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">ThinKing
	</td>
	<td style=\"vertical-align: top;\">ivybridge
	</td>
	<td style=\"vertical-align: top;\">Xeon E5-2680v2
	</td>
	<td style=\"vertical-align: top;\">IB-QDR
	</td>
	<td style=\"vertical-align: top;\">20
	</td>
	<td style=\"vertical-align: top;\">128 GB
	</td>
	<td style=\"vertical-align: top;\">124 GB
	</td>
	<td style=\"vertical-align: top;\">250 GB
	</td>
	<td style=\"vertical-align: top;\">32
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">Thinking
	</td>
	<td style=\"vertical-align: top;\">haswell
	</td>
	<td style=\"vertical-align: top;\">Xeon E5-2680v3
	</td>
	<td style=\"vertical-align: top;\">IB-FDR
	</td>
	<td style=\"vertical-align: top;\">24
	</td>
	<td style=\"vertical-align: top;\">64 GB
	</td>
	<td style=\"vertical-align: top;\">60 GB
	</td>
	<td style=\"vertical-align: top;\">150 GB
	</td>
	<td style=\"vertical-align: top;\">48<br>
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">Thinking
	</td>
	<td style=\"vertical-align: top;\">haswell
	</td>
	<td style=\"vertical-align: top;\">Xeon E5-2680v3
	</td>
	<td style=\"vertical-align: top;\">IB-FDR
	</td>
	<td style=\"vertical-align: top;\">24
	</td>
	<td style=\"vertical-align: top;\">128 GB
	</td>
	<td style=\"vertical-align: top;\">124 GB
	</td>
	<td style=\"vertical-align: top;\">150 GB
	</td>
	<td style=\"vertical-align: top;\">96<br>
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">Genius<br>
	</td>
	<td style=\"vertical-align: top;\">skylake
	</td>
	<td style=\"vertical-align: top;\">Xeon 6140
	</td>
	<td style=\"vertical-align: top;\">IB-EDR
	</td>
	<td style=\"vertical-align: top;\">36
	</td>
	<td style=\"vertical-align: top;\">192 GB
	</td>
	<td style=\"vertical-align: top;\">188 GB
	</td>
	<td style=\"vertical-align: top;\">800 GB
	</td>
	<td style=\"vertical-align: top;\">86<br>
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">Genius
	</td>
	<td style=\"vertical-align: top;\">skylake-large memory
	</td>
	<td style=\"vertical-align: top;\">Xeon 6140
	</td>
	<td style=\"vertical-align: top;\">IB-EDR
	</td>
	<td style=\"vertical-align: top;\">36
	</td>
	<td style=\"vertical-align: top;\">768 GB
	</td>
	<td style=\"vertical-align: top;\">764 GB
	</td>
	<td style=\"vertical-align: top;\">800 GB
	</td>
	<td style=\"vertical-align: top;\">10<br>
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">Genius
	</td>
	<td style=\"vertical-align: top;\">skylake-GPU
	</td>
	<td style=\"vertical-align: top;\">Xeon 6140<br>4xP100 SXM2<br>
	</td>
	<td style=\"vertical-align: top;\">IB-EDR
	</td>
	<td style=\"vertical-align: top;\">36
	</td>
	<td style=\"vertical-align: top;\">192 GB
	</td>
	<td style=\"vertical-align: top;\">188 GB
	</td>
	<td style=\"vertical-align: top;\">800 GB
	</td>
	<td style=\"vertical-align: top;\">20<br>
	</td>
</tr>
</tbody>
</table>
<p>For using Cerebro, the shared memory section, we refer to the <a href=\"/assets/965\">Cerebro Quick Start Guide</a>.
</p>
<h2>Implementation of the VSC directory structure</h2>
<p>In the transition phase between Vic3 and ThinKing, the storage is mounted on both systems. When switching from Vic3 to ThinKing you will not need to migrate your data.
</p>
<p>The cluster uses the directory structure that is implemented on most VSC clusters. This implies that each user has two personal directories:
</p>
<ul>
	<li>A regular home directory which contains all files that a user might need to log on to the system, and small 'utility' scripts/programs/source code/.... The capacity that can be used is restricted by quota and this directory should not be used for I/O intensive programs. <br>
	For KU Leuven systems the full path is of the form /user/leuven/... , but this might be different on other VSC systems. However, on all systems, the environment variable VSC_HOME points to this directory (just as the standard HOME variable does).</li>
	<li>A data directory which can be used to store programs and their results. At the moment, there are no quota on this directory. For KU Leuven the path name is /data/leuven/... . On all VSC systems, the environment variable VSC_DATA points to this directory.</li>
</ul>
<p>There are three further environment variables that point to other directories that can be used:
</p>
<ul>
	<li>On each cluster you have access to a scratch directory that is shared by all nodes on the cluster. The variable VSC_SCRATCH_SITE will point to this directory. This directory is also accessible from the loginnodes, so it is accessible while your jobs run, and after they finish (for a limited time: files can be removed automatically after 14 days.)</li>
	<li>Similarly, on each cluster you have a VSC_SCRATCH_NODE directory, which is a scratch space local to each computenode. Thus, on each node, this directory point to a different physical location, and the connects are only accessible from that particular worknode, and (typically) only during the runtime of your job. But, if more than one job of you runs on the same node, they all see the same directory (and thus you have to make sure they do not overwrite each others data by creating subdirectories per job, or give proper filename, ...)</li>
</ul>
<h2></h2>
<h2>Access restrictions<a id=\"AccessRestrictions\" name=\"AccessRestrictions\"></a></h2>
<p>Access
 is available for faculty, students (under faculty supervision), and 
researchers of the KU  Leuven, UHasselt and their associations. This 
cluster is being integrated in the VSC network and as such becomes 
available to all VSC users.
</p>
<h2>History</h2>
<p>In September 2013 a new thin node cluster (HP) and a shared memory system (SGI) was bought. The thin node cluster was installed and configured in January/February 2014 and extended in september 2014. Installation and configuration of the SMP is done in April 2014. Financing of this systems was obtained from the Hercules foundation and the Flemish government.
</p>
<p>Do you want to see it ? Have a look at the movie
</p>
<p>
	<iframe allowfullscreen=\"\" src=\"//www.youtube.com/embed/O4jOzReDYnc?rel=0\" width=\"640\" height=\"360\" frameborder=\"0\">
	</iframe>
</p>"

