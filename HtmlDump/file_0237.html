<h2>Purpose</h2><p>MPI is a language-independent communications protocol used to program parallel computers. Both point-to-point and collective communication are supported. MPI \"is a message-passing application programmer interface, together with protocol and semantic specifications for how its features must behave in any implementation.\" MPI's goals are high performance, scalability, and portability. MPI remains the dominant model used in high-performance computing today.
</p>
<p>The current version of the MPI standard is 3.0, but only the newest implementations implement the full standard. The previous specifications are the MPI 2.0 specification with minor updates in the MPI-2.1 and MPI-2.2 specifications. The standardisation body for MPI is the <a href=\"https://www.mpi-forum.org/\" target=\"_blank\">MPI forum</a>.
</p>
<h2>Some background information</h2>
<p>MPI-1.0 (1994) and its updates MPI-1.1 (1995), MPI-1.2 (1997) and MPI-1.3 (1998) concentrate on point-to-point communication (send/receive) and global operations in a static process topology. Major additions in MPI-2.0 (1997) and its updates MPI-2.1 (2008) and MPI-2.2 (2009) are one-sided communication (get/put), dynamic process management and a model for parallel I/O. MPI-3.0 (2012) adds non-blocking collectives, a major update of the one-sided communication model and neighbourhood collectives on graph topologies. The first update of the MPI-3.1 specification was released in 2015, and work is ongoing on the next major update, MPI-4.0.
</p>
<p>The two dominant Open Source implementations are <a href=\"https://www.open-mpi.org/\" target=\"_blank\">Open MPI</a> and <a href=\"https://www.mpich.org/\" target=\"_blank\">MPICH</a>. The latter has been through a couple of name changes: It was originally conceived in the early '90's as MPICH, then the complete rewrite was renamed to MPICH2, but as this name caused confusion as the MPI standard evolved into MPI 3.x, the name was changed again to MPICH, and the version number bumped to 3.0. MVAPICH developed at Ohio State University is the offspring of MPICH further optimised for InfiniBand and some other high-performance interconnect technologies. Most other MPI implementations are derived from one of these implementations.
</p>
<p>At the VSC we offer both implementations: Open MPI is offered with the GNU compilers in the <a href=\"/cluster-doc/development/toolchain-foss\">FOSS toolchain</a>, while the Intel MPI used in the <a href=\"/cluster-doc/development/toolchain-intel\">Intel toolchain</a> is derived from the MPICH code base.
</p>
<h2>Prerequisites</h2>
<p>You have a program that uses an MPI library, either developed by you, or by others. In the latter case, the program's documentation should mention the MPI library it was developed with.
</p>
<h2>Implementations</h2>
<p>On VSC clusters, several MPI implementations are installed. We provide two MPI implementations on all newer machines that can support those implementations:
</p>
<ol>
	<li><a href=\"/cluster-doc/development/toolchain-intel#intel-mpi\">Intel MPI</a> in the intel toolchain
	<ol>
		<li>Intel MPI 4.1 (intel/2014a and intel/2014b toolchains) implements the MPI-2.2 specification</li>
		<li>Intel MPI 5.0 (intel/2015a and intel/2015b toolchains) and Intel MPI 5.1 (intel/2016a and intel/2016b toolchains) implement the MPI-3.0 specification</li>
	</ol>
	</li>
	<li><a href=\"/cluster-doc/development/toolchain-foss#openmpi\">Open MPI</a> in the foss toolchain
	<ol>
		<li>Open MPI 1.6 (foss/2014a toolchain) only implements the MPI-2.1 specification</li>
		<li>Open MPI 1.8 (foss/2014b, foss/2015a and foss/2015b toolchains) and Open MPI 1.10 (foss/2016a and foss/2016b) implement the MPI-3.0 specification</li>
	</ol>
	</li>
</ol>
<p><span style=\"line-height: 1.5em;\">When developing your own software, this is the preferred order to select an implementation. The performance should be very similar, however, more development tools are available for Intel MPI (i.e., </span><a href=\"/cluster-doc/development/itac\">ITAC</a> for performance monitoring<span style=\"line-height: 1.5em;\">).</span>
</p>
<p><span style=\"line-height: 1.5em;\">Specialised hardware sometimes requires specialised MPI-libraries.</span>
</p>
<ul>
	<li><span style=\"line-height: 1.5em;\">The interconnect in <a href=\"/infrastructure/hardware/hardware-kul#Cerebro\">Cerebro, the SGI UV shared memory machine at KU Leuven</a>, provides hardware acceleration for some MPI functions. To take full advantage of the interconnect, it is necessary to use the SGI MPI library, part of the MPT packages which stands for Message Passing Toolkit (and also contains SGI's own implementation of <a href=\"http://www.openshmem.org/site/\" target=\"_blank\">OpenSHMEM</a>). Support is offered through additional toolchains (intel-mpt and foss-mpt).</span>
	<ul>
		<li><span style=\"line-height: 1.5em;\">SGI MPT 2.09 (intel-mpt/2014a and foss-mpt/2014a toolchains) contains the SGI MPI 1.7 library which implements the MPI-2.2 specification.</span></li>
		<li><span style=\"line-height: 1.5em;\">SGI MPT 2.10 (not yet installed, contact <a href=\"/support/contact-support\">KU Leuven support</a>) contains the SGI MPI 1.8 library which implements the MPI-3.0 specification.</span></li>
	</ul>
	</li>
</ul>
<p>Several other implementations may be installed, e.g., <a href=\"http://mvapich.cse.ohio-state.edu/\" target=\"_blank\">MVAPICH</a>, but we assume you know what you're doing if you choose to use them.
</p>
<p>We also assume you are already familiar with the job submission procedure. If not, check the \"<a href=\"/cluster-doc/running-jobs\">Running jobs</a>\" section first.
</p>
<h2>Compiling and running</h2>
<p>See to the documentation about the <a href=\"/cluster-doc/development/toolchains\">toolchains</a>.
</p>
<h2>Debugging</h2>
<p>For debugging, we recommend the ARM DDT debugger (formerly Allinea DDT, module allinea-ddt). <a href=\"https://developer.arm.com/products/software-development-tools/hpc/arm-forge/arm-ddt/video-demos-and-tutorials-for-arm-ddt\">Video tutorials are available on the Arm web site</a>. (KU Leuven-only).
</p>
<p>When using the intel toolchain, <a href=\"/cluster-doc/development/itac\">Intel's Trace Analyser & Collector</a> (ITAC) may also prove useful.
</p>
<h2>Profiling</h2>
<p>To profile MPI applications, one may use <a href=\"https://www.arm.com/products/development-tools/hpc-tools/cross-platform/forge/map\" target=\"_blank\">Arm MAP (formerly Allinea MAP)</a>, or <a href=\"http://www.scalasca.org/software/scalasca-2.x/documentation.html\" target=\"_blank\">Scalasca</a>. (KU Leuven-only)
</p>
<h2>Further information</h2>
<ul>
	<li><a href=\"https://software.intel.com/en-us/intel-mpi-library\" target=\"_blank\">Intel MPI</a>
	<ul>
		<li><a href=\"https://software.intel.com/en-us/articles/intel-mpi-library-documentation/\" target=\"_blank\">Documentation</a> (Latest version)</li>
	</ul>
	</li>
	<li><a href=\"https://www.open-mpi.org/\" target=\"_blank\">Open MPI</a>
	<ul>
		<li><a href=\"https://www.open-mpi.org/doc/\" target=\"_blank\">Documentation</a></li>
	</ul>
	</li>
	<li>SGI MPT, now HPE Performance Software MPI
	<ul>
		<li><a href=\"https://support.hpe.com/hpsc/doc/public/display?docId=emr_na-a00037728en_us&docLocale=en_US\" target=\"_blank\">Documentation</a></li>
	</ul>
	</li>
	<li><a href=\"https://www.mpi-forum.org\" target=\"_blank\">MPI forum</a>, where you can also find the standard specifications
	<ul>
		<li><a href=\"https://www.mpi-forum.org/docs/\" target=\"_blank\">Standard documents</a></li>
	</ul>
	</li>
	<li>See also the pages in the <a href=\"/support/tut-book\">tutorials section</a>, e.g., for <a href=\"/support/tut-book/books#MPI\">books</a> and <a href=\"/support/tut-book/web-tutorials\">online tutorial</a></li>
</ul>"

