<p>Data on the VSC clusters can be stored in several locations, depending on the size and usage of these data. Following locations are available:</p>
<ul>
    <li><a href=\"#Home\">Home directory</a>

    <ul>
        <li>Location available as $VSC_HOME</li>
        <li>The data stored here should be relatively small (e.g., no files or directories larger than a gigabyte, although this is allowed), and not generating very intense I/O during jobs.&nbsp;<br />
        Also all kinds of configuration files are stored here, e.g., ssh-keys, .bashrc, or Matlab, and Eclipse configuration, ...</li>
    </ul>
    </li>
    <li><a href=\"#Data\">Data directory</a>
    <ul>
        <li>Location available as $VSC_DATA</li>
        <li>A bigger 'workspace', for datasets, results, logfiles, ... . This filesystem can be used for higher I/O loads, but for I/O bound jobs, you might be better of using one of the 'scratch' filesystems.</li>
    </ul>
    </li>
    <li><a href=\"#Scratch\">Scratch directories</a>
    <ul>
        <li>Several types exist, available in $VSC_SCRATCH_XXX variables</li>
        <li>For temporary or transient data; there is typically no backup for these filesystems, and 'old' data may be removed automatically.</li>
        <li>Currently, $VSC_SCRATCH_NODE, $VSC_SCRATCH_SITE and $VSC_SCRATCH_GLOBAL are defined, for space that is available per node, per site, or globally on all nodes of the VSC (currenlty, there is no real 'global' scratch filesystem yet).</li>
    </ul>
    </li>
</ul>

<p>Since these directories are not necessarily mounted on the same locations over all sites, you should always (try to) use the environment variables that have been created.</p>

<p>Quota is enabled on the three directories, which means the amount of data you can store here is limited by the operating system, and not by \"the boundaries of the hard disk\". You can see your current usage and the current limits with the appropriate quota command as explained on <a href=\"/cluster-doc/check-disk-usage\">How do I know how much disk space I am using?</a>. The actual disk capacity, shared by <i>all</i> users, can be found on the&nbsp; <a href=\"/infrastructure/hardware\">Available hardware</a> page.</p>

<p>You will only receive a warning when you reach the soft limit of either quota. You will only start losing data when you reach the hard limit. Data loss occurs when you try to save new files: this will not work because you have no space left, and thus you will lose these new files. You will however not be warned when data loss occurs, so keep an eye open for the general quota warnings! The same holds for running jobs that need to write files: when you reach your hard quota, jobs will crash.</p>

<h2><a name=\"Home\"></a>Home directory</h2>

<p>This directory is where you arrive by default when you login to the cluster. Your shell refers to it as \"~\" (tilde), or via the environment variable $VSC_HOME.</p>

<p>The data stored here should be relatively small (e.g., no files or directories larger than a gigabyte, although this is allowed), and usually used frequently. Also all kinds of configuration files are stored here, e.g., by Matlab, Eclipse, ...</p>

<p>The operating system also creates a few files and folders here to manage your account. Examples are:</p>

<table class=\"prettytable\">
    <tbody>
        <tr>
            <td>.ssh/</td>
            <td>This directory contains some files necessary for you to login to the cluster and to submit jobs on the cluster. Do not remove them, and do not alter anything if you don't know what you're doing!</td>
        </tr>
        <tr>
            <td>.profile</td>
            <td>This script defines some general settings about your sessions,</td>
        </tr>
        <tr>
            <td>.bashrc</td>
            <td>This script is executed everytime you start a session on the cluster: when you login to the cluster and when a job starts. You could edit this file and, e.g., add \"module load XYZ\" if you want to automatically load module XYZ whenever you login to the cluster, although we do not recommend to load modules in your .bashrc.</td>
        </tr>
        <tr>
            <td>.bash_history</td>
            <td>This file contains the commands you typed at your shell prompt, in case you need them again.</td>
        </tr>
    </tbody>
</table>

<h2><a name=\"Data\"></a>Data directory</h2>

<p>In this directory you can store all other data that you need for longer terms. The environment variable pointing to it is $VSC_DATA. There are no guarantees about the speed you'll achieve on this volume.</p>

<h2><a name=\"Scratch\"></a>Scratch space</h2>

<p>To enable quick writing from your job, a few extra file systems are available on the work nodes. These extra file systems are called scratch folders, and can be used for storage of temporary and/or transient data (temporary results, anything you just need during your job, or your batch of jobs).</p>

<p>You should remove any data from these systems after your processing them has finished. There are no gurarantees about the time your data will be stored on this system, and we plan to clean these automatically on a regular base. The maximum allowed age of files on these scratch file systems depends on the type of scratch, and can be anywhere between a day and a few weeks. We don't guarantee that these policies remain forever, and may change them if this seems necessary for the healthy operation of the cluster.</p>

<p>Each type of scratch has his own use:</p>

<ul>
    <li><strong>Node scratch ($VSC_SCRATCH_NODE)</strong><br />
    Every node has its own scratch space, which is completely seperated from the other nodes. Every job automatically gets its own temporary directory on this node scratch, available through the environment variable $TMPDIR. $TMPDIR is guaranteed to be unique for each job.<br />
    Note however that when your job requests multiple cores and these cores happen to be in the same node, this $TMPDIR is shared among the cores!</li>
    <li><strong>Site scratch ($VSC_SCRATCH_SITE, $VSC_SCRATCH)</strong><br />
    To allow a job running on multiple nodes (or multiple jobs running on seperate nodes) to share data as files, every node of the cluster (including the login nodes) has access to this shared scratch directory. Just like the home and data directories, every user has its own scratch directory. Because this scratch is also available from the login nodes, you could manually copy results to your data directory after your job has ended.</li>
    <li><strong>Global scratch ($VSC_SCRATCH_GLOBAL)</strong><br />
    In the long term, this scratch space will be available throughout the whole VSC. At the time of writing, the global scratch is just the same volume as the site scratch, and thus contains the same data.</li>
</ul>"

